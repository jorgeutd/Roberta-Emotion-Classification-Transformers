{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# If there\"s a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4173f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"sagemaker==2.69.0\" \"transformers==4.12.3\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7942e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "# assert sagemaker.__version__ >= \"2.69.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4658fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.68.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd9cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bff31ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::197614225699:role/bi-sagemaker-access\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "print(f\"sagemaker role arn: {role}\")\n",
    "# print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab75c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='emotion-class-197614225699'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d24b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path = f's3://{bucket}/processing_output/train_data'\n",
    "val_input_path = f's3://{bucket}/processing_output/validation_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3721ba97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://emotion-class-197614225699/processing_output/validation_data'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6814c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d60892",
   "metadata": {},
   "source": [
    "### Set up Huggingface training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08e79514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 10,\n",
    "                 'train_batch_size': 16,\n",
    "                 'learning_rate': 3e-5, \n",
    "                 'fp16': True,\n",
    "                 'model_name':'roberta-base',\n",
    "                 'weight_decay':0.01,\n",
    "                 'do_eval': True,\n",
    "                 'load_best_model_at_end':True\n",
    "                 }\n",
    "\n",
    "# #configuration for running training on smdistributed Data Parallel\n",
    "# distribution = {'smdistributed':{'dataparallel':{ 'enabled': True}}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4c2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### check training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0687b435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score, precision_recall_fscore_support,roc_auc_score\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m5\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--fp16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\r\n",
      "    \r\n",
      "        \r\n",
      "\u001b[37m#     # Push to Hub Parameters\u001b[39;49;00m\r\n",
      "\u001b[37m#     parser.add_argument(\"--push_to_hub\", type=bool, default=True)\u001b[39;49;00m\r\n",
      "\u001b[37m#     parser.add_argument(\"--hub_model_id\", type=str, default=None)\u001b[39;49;00m\r\n",
      "\u001b[37m#     parser.add_argument(\"--hub_strategy\", type=str, default=None)\u001b[39;49;00m\r\n",
      "\u001b[37m#     parser.add_argument(\"--hub_token\", type=str, default=None)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--val_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_VAL\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \r\n",
      "\r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "    \r\n",
      "    os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mGPU_NUM_DEVICES\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]=args.n_gpus\r\n",
      "\r\n",
      "    \u001b[37m# Set up logging\u001b[39;49;00m\r\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "\r\n",
      "    logging.basicConfig(\r\n",
      "        level=logging.getLevelName(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\r\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\r\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[37m#     # make sure we have required parameters to push\u001b[39;49;00m\r\n",
      "\u001b[37m#     if args.push_to_hub:\u001b[39;49;00m\r\n",
      "\u001b[37m#         if args.hub_strategy is None:\u001b[39;49;00m\r\n",
      "\u001b[37m#             raise ValueError(\"--hub_strategy is required when pushing to Hub\")\u001b[39;49;00m\r\n",
      "\u001b[37m#         if args.hub_token is None:\u001b[39;49;00m\r\n",
      "\u001b[37m#             raise ValueError(\"--hub_token is required when pushing to Hub\")\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m#     # sets hub id if not provided\u001b[39;49;00m\r\n",
      "\u001b[37m#     if args.hub_model_id is None:\u001b[39;49;00m\r\n",
      "\u001b[37m#         args.hub_model_id = args.model_id.replace(\"/\", \"--\")\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# load datasets\u001b[39;49;00m\r\n",
      "    train_dataset = load_from_disk(args.training_dir)\r\n",
      "    val_dataset = load_from_disk(args.val_dir)\r\n",
      "\r\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded train_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(train_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded test_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(val_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "     \u001b[37m# Prepare model labels - useful in inference API\u001b[39;49;00m\r\n",
      "    labels = train_dataset.features[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].names\r\n",
      "    num_labels = \u001b[36mlen\u001b[39;49;00m(labels)\r\n",
      "    label2id, id2label = \u001b[36mdict\u001b[39;49;00m(), \u001b[36mdict\u001b[39;49;00m()\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(labels):\r\n",
      "        label2id[label] = \u001b[36mstr\u001b[39;49;00m(i)\r\n",
      "        id2label[\u001b[36mstr\u001b[39;49;00m(i)] = label\r\n",
      "\r\n",
      "    \u001b[37m# compute metrics function for binary classification\u001b[39;49;00m\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(pred):\r\n",
      "        labels = pred.label_ids\r\n",
      "        preds = pred.predictions.argmax(-\u001b[34m1\u001b[39;49;00m)\r\n",
      "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\u001b[33m\"\u001b[39;49;00m\u001b[33mweighted\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        acc = accuracy_score(labels, preds)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: acc, \u001b[33m\"\u001b[39;49;00m\u001b[33mf1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: f1, \u001b[33m\"\u001b[39;49;00m\u001b[33mprecision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: precision, \u001b[33m\"\u001b[39;49;00m\u001b[33mrecall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: recall}\r\n",
      "    \r\n",
      "\r\n",
      "\r\n",
      "    \u001b[37m# download model from model hub\u001b[39;49;00m\r\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(\r\n",
      "        args.model_name, num_labels=num_labels, label2id=label2id, id2label=id2label\r\n",
      "    )\r\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\r\n",
      "\r\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\r\n",
      "    training_args = TrainingArguments(\r\n",
      "        output_dir=args.model_dir,\r\n",
      "        num_train_epochs=args.epochs,\r\n",
      "        per_device_train_batch_size=args.train_batch_size,\r\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\r\n",
      "        warmup_steps=args.warmup_steps,\r\n",
      "        fp16=args.fp16,\r\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        save_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        save_total_limit=\u001b[34m1\u001b[39;49;00m,\r\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\r\n",
      "        load_best_model_at_end=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "        metric_for_best_model=\u001b[33m\"\u001b[39;49;00m\u001b[33mf1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[37m#         # push to hub parameters\u001b[39;49;00m\r\n",
      "\u001b[37m#         push_to_hub=args.push_to_hub,\u001b[39;49;00m\r\n",
      "\u001b[37m#         hub_strategy=args.hub_strategy,\u001b[39;49;00m\r\n",
      "\u001b[37m#         hub_model_id=args.hub_model_id,\u001b[39;49;00m\r\n",
      "\u001b[37m#         hub_token=args.hub_token,\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\r\n",
      "    trainer = Trainer(\r\n",
      "        model=model,\r\n",
      "        args=training_args,\r\n",
      "        compute_metrics=compute_metrics,\r\n",
      "        train_dataset=train_dataset,\r\n",
      "        eval_dataset=val_dataset,\r\n",
      "        tokenizer=tokenizer,\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[37m# train model\u001b[39;49;00m\r\n",
      "    trainer.train()\r\n",
      "\r\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\r\n",
      "    eval_result = trainer.evaluate(eval_dataset=val_dataset)\r\n",
      "\r\n",
      "\u001b[37m#     # save best model, metrics and create model card\u001b[39;49;00m\r\n",
      "\u001b[37m#     trainer.create_model_card(model_name=args.hub_model_id)\u001b[39;49;00m\r\n",
      "\u001b[37m#     trainer.push_to_hub()\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Saves the model to s3 uses os.environ[\"SM_MODEL_DIR\"] to make sure checkpointing works\u001b[39;49;00m\r\n",
      "    trainer.save_model(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\r\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Saves the model to s3\u001b[39;49;00m\r\n",
      "    trainer.save_model(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./scripts/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60808413",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.p3.16xlarge',\n",
    "    instance_count=1,\n",
    "    volume_size=1000,\n",
    "    role=role,\n",
    "    transformers_version = '4.12',           # the transformers version used in the training job\n",
    "    pytorch_version      = '1.9',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py38',            # the python version used in the training job\n",
    "    output_path=f's3://{bucket}/training_output/',\n",
    "    base_job_name=\"emotion-training\",\n",
    "    hyperparameters=hyperparameters,\n",
    "#     disable_profiler=True,\n",
    "#     distribution=distribution,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98ac9a8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-03 20:58:03 Starting - Starting the training job...\n",
      "2021-12-03 20:58:11 Starting - Launching requested ML instancesProfilerReport-1638565083: InProgress\n",
      ".........\n",
      "2021-12-03 20:59:54 Starting - Preparing the instances for training.........\n",
      "2021-12-03 21:01:30 Downloading - Downloading input data...\n",
      "2021-12-03 21:01:50 Training - Downloading the training image...........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-12-03 21:06:25,475 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-12-03 21:06:25,550 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-12-03 21:06:26,968 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-12-03 21:06:27,756 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"val\": \"/opt/ml/input/data/val\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"load_best_model_at_end\": true,\n",
      "        \"weight_decay\": 0.01,\n",
      "        \"train_batch_size\": 16,\n",
      "        \"do_eval\": true,\n",
      "        \"model_name\": \"roberta-base\",\n",
      "        \"epochs\": 10,\n",
      "        \"learning_rate\": 3e-05,\n",
      "        \"fp16\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"emotion-training-2021-12-03-20-58-03-225\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://emotion-class-197614225699/emotion-training-2021-12-03-20-58-03-225/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"epochs\":10,\"fp16\":true,\"learning_rate\":3e-05,\"load_best_model_at_end\":true,\"model_name\":\"roberta-base\",\"train_batch_size\":16,\"weight_decay\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://emotion-class-197614225699/emotion-training-2021-12-03-20-58-03-225/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"epochs\":10,\"fp16\":true,\"learning_rate\":3e-05,\"load_best_model_at_end\":true,\"model_name\":\"roberta-base\",\"train_batch_size\":16,\"weight_decay\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"emotion-training-2021-12-03-20-58-03-225\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://emotion-class-197614225699/emotion-training-2021-12-03-20-58-03-225/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--epochs\",\"10\",\"--fp16\",\"True\",\"--learning_rate\",\"3e-05\",\"--load_best_model_at_end\",\"True\",\"--model_name\",\"roberta-base\",\"--train_batch_size\",\"16\",\"--weight_decay\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=true\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=3e-05\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --do_eval True --epochs 10 --fp16 True --learning_rate 3e-05 --load_best_model_at_end True --model_name roberta-base --train_batch_size 16 --weight_decay 0.01\u001b[0m\n",
      "\u001b[34m2021-12-03 21:06:32,435 - __main__ - INFO -  loaded train_dataset length is: 16000\u001b[0m\n",
      "\u001b[34m2021-12-03 21:06:32,435 - __main__ - INFO -  loaded test_dataset length is: 2000\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 481/481 [00:00<00:00, 668kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/478M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 5.60M/478M [00:00<00:08, 58.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 11.9M/478M [00:00<00:07, 62.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 18.1M/478M [00:00<00:07, 64.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 24.4M/478M [00:00<00:07, 64.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▋         | 30.7M/478M [00:00<00:07, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 37.0M/478M [00:00<00:07, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 43.2M/478M [00:00<00:06, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 49.5M/478M [00:00<00:06, 65.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 55.8M/478M [00:00<00:06, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 62.1M/478M [00:01<00:06, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 68.4M/478M [00:01<00:06, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 74.7M/478M [00:01<00:06, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 81.0M/478M [00:01<00:06, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 87.3M/478M [00:01<00:06, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|█▉        | 93.5M/478M [00:01<00:06, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 99.8M/478M [00:01<00:06, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 106M/478M [00:01<00:05, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▎       | 112M/478M [00:01<00:05, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▍       | 119M/478M [00:01<00:05, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 125M/478M [00:02<00:05, 66.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 131M/478M [00:02<00:05, 66.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 138M/478M [00:02<00:05, 66.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 144M/478M [00:02<00:05, 66.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███▏      | 150M/478M [00:02<00:05, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 157M/478M [00:02<00:05, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 163M/478M [00:02<00:04, 66.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▌      | 169M/478M [00:02<00:04, 66.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 176M/478M [00:02<00:04, 66.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 182M/478M [00:02<00:04, 66.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 188M/478M [00:03<00:04, 66.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 195M/478M [00:03<00:04, 66.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 201M/478M [00:03<00:04, 66.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 207M/478M [00:03<00:04, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 214M/478M [00:03<00:04, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 220M/478M [00:03<00:04, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 226M/478M [00:03<00:04, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▊     | 233M/478M [00:03<00:03, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 239M/478M [00:03<00:03, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████▏    | 245M/478M [00:03<00:03, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 252M/478M [00:04<00:03, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 258M/478M [00:04<00:03, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 264M/478M [00:04<00:03, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 271M/478M [00:04<00:03, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 277M/478M [00:04<00:03, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 283M/478M [00:04<00:03, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 289M/478M [00:04<00:02, 66.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 296M/478M [00:04<00:02, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 302M/478M [00:04<00:02, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▍   | 308M/478M [00:04<00:02, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 315M/478M [00:05<00:02, 59.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 321M/478M [00:05<00:02, 60.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 327M/478M [00:05<00:02, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 333M/478M [00:05<00:02, 63.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 340M/478M [00:05<00:02, 64.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 346M/478M [00:05<00:02, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▎  | 352M/478M [00:05<00:02, 64.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▌  | 358M/478M [00:05<00:01, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▋  | 365M/478M [00:05<00:01, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 371M/478M [00:05<00:01, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 377M/478M [00:06<00:01, 65.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|████████  | 384M/478M [00:06<00:01, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 390M/478M [00:06<00:01, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 396M/478M [00:06<00:01, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 403M/478M [00:06<00:01, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 409M/478M [00:06<00:01, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 415M/478M [00:06<00:01, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 421M/478M [00:06<00:00, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 428M/478M [00:06<00:00, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 434M/478M [00:06<00:00, 66.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 440M/478M [00:07<00:00, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 447M/478M [00:07<00:00, 66.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 453M/478M [00:07<00:00, 66.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 459M/478M [00:07<00:00, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 466M/478M [00:07<00:00, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▊| 472M/478M [00:07<00:00, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 478M/478M [00:07<00:00, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/878k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 878k/878k [00:00<00:00, 51.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/446k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 446k/446k [00:00<00:00, 55.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 55.2MB/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mUsing amp fp16 backend\u001b[0m\n",
      "\u001b[34mUsing amp fp16 backend\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 16000\n",
      "  Num Epochs = 10\u001b[0m\n",
      "\u001b[34mNum examples = 16000\n",
      "  Num Epochs = 10\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1250\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1250\u001b[0m\n",
      "\u001b[34m0%|          | 0/1250 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.000 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.126 algo-1:26 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.126 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.127 algo-1:26 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.127 algo-1:26 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.127 algo-1:26 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.359 algo-1:26 INFO hook.py:591] name:module.roberta.embeddings.word_embeddings.weight count_params:38603520\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.360 algo-1:26 INFO hook.py:591] name:module.roberta.embeddings.position_embeddings.weight count_params:394752\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.360 algo-1:26 INFO hook.py:591] name:module.roberta.embeddings.token_type_embeddings.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.360 algo-1:26 INFO hook.py:591] name:module.roberta.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.360 algo-1:26 INFO hook.py:591] name:module.roberta.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.360 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.360 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.360 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.360 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.360 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.360 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.360 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.361 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.362 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.363 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.364 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.365 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.366 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.367 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.368 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.369 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.370 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.371 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.372 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.373 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.374 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.375 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.375 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.375 algo-1:26 INFO hook.py:591] name:module.roberta.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.375 algo-1:26 INFO hook.py:591] name:module.classifier.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.375 algo-1:26 INFO hook.py:591] name:module.classifier.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.375 algo-1:26 INFO hook.py:591] name:module.classifier.out_proj.weight count_params:4608\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.375 algo-1:26 INFO hook.py:591] name:module.classifier.out_proj.bias count_params:6\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.375 algo-1:26 INFO hook.py:593] Total Trainable Params: 124650246\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.375 algo-1:26 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-12-03 21:06:49.381 algo-1:26 INFO hook.py:488] Hook is writing from the hook with pid: 26\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-12-03 21:06:52 Training - Training image download completed. Training in progress.\u001b[34malgo-1:26:26 [0] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34mNCCL version 2.7.8+cuda11.1\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m0%|          | 1/1250 [00:21<7:32:33, 21.74s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/1250 [00:22<3:12:28,  9.25s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/1250 [00:22<1:49:08,  5.25s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/1250 [00:23<1:10:03,  3.37s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/1250 [00:23<49:18,  2.38s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 6/1250 [00:24<35:51,  1.73s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 7/1250 [00:24<27:20,  1.32s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 8/1250 [00:25<21:45,  1.05s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 9/1250 [00:25<18:00,  1.15it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 10/1250 [00:26<15:51,  1.30it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 11/1250 [00:26<13:59,  1.48it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 12/1250 [00:27<12:42,  1.62it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 13/1250 [00:27<11:48,  1.75it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 14/1250 [00:28<11:10,  1.84it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 15/1250 [00:28<11:04,  1.86it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 16/1250 [00:29<10:40,  1.93it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 17/1250 [00:29<10:23,  1.98it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 18/1250 [00:30<10:13,  2.01it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 19/1250 [00:30<10:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 20/1250 [00:31<10:19,  1.98it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 21/1250 [00:31<10:11,  2.01it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 22/1250 [00:32<10:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 23/1250 [00:32<10:02,  2.04it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 24/1250 [00:33<10:19,  1.98it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 25/1250 [00:33<10:09,  2.01it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 26/1250 [00:34<10:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 27/1250 [00:34<09:59,  2.04it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 28/1250 [00:35<09:57,  2.05it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 29/1250 [00:35<10:13,  1.99it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 30/1250 [00:36<10:02,  2.03it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 31/1250 [00:36<09:57,  2.04it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 32/1250 [00:37<09:51,  2.06it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 33/1250 [00:37<09:48,  2.07it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 34/1250 [00:38<10:05,  2.01it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 35/1250 [00:38<09:58,  2.03it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 36/1250 [00:39<09:54,  2.04it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 37/1250 [00:39<09:51,  2.05it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 38/1250 [00:39<09:50,  2.05it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 39/1250 [00:40<10:09,  1.99it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 40/1250 [00:40<09:58,  2.02it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 41/1250 [00:41<09:51,  2.04it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 42/1250 [00:41<09:44,  2.07it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 43/1250 [00:42<10:04,  2.00it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 44/1250 [00:42<09:55,  2.02it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 45/1250 [00:43<09:48,  2.05it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 46/1250 [00:43<09:58,  2.01it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 47/1250 [00:44<09:57,  2.01it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 48/1250 [00:44<10:09,  1.97it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 49/1250 [00:45<10:00,  2.00it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 50/1250 [00:45<09:54,  2.02it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 51/1250 [00:46<09:48,  2.04it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 52/1250 [00:46<09:44,  2.05it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 53/1250 [00:47<10:02,  1.99it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 54/1250 [00:47<09:56,  2.00it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 55/1250 [00:48<09:50,  2.03it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 56/1250 [00:48<09:45,  2.04it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 57/1250 [00:49<10:01,  1.98it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 58/1250 [00:49<09:54,  2.01it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 59/1250 [00:50<09:49,  2.02it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 60/1250 [00:50<09:44,  2.04it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 61/1250 [00:51<09:42,  2.04it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 62/1250 [00:51<10:05,  1.96it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 63/1250 [00:52<09:57,  1.99it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 64/1250 [00:52<09:52,  2.00it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 65/1250 [00:53<09:48,  2.01it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 66/1250 [00:53<09:45,  2.02it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 67/1250 [00:54<10:05,  1.95it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 68/1250 [00:54<09:58,  1.98it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 69/1250 [00:55<09:50,  2.00it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 70/1250 [00:55<09:46,  2.01it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 71/1250 [00:56<09:42,  2.02it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 72/1250 [00:56<10:01,  1.96it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 73/1250 [00:57<09:54,  1.98it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 74/1250 [00:57<09:50,  1.99it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 75/1250 [00:58<09:44,  2.01it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 76/1250 [00:58<10:01,  1.95it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 77/1250 [00:59<09:54,  1.97it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 78/1250 [00:59<09:49,  1.99it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 79/1250 [01:00<09:42,  2.01it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 80/1250 [01:00<09:38,  2.02it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 81/1250 [01:01<10:04,  1.93it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 82/1250 [01:01<09:55,  1.96it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 83/1250 [01:02<09:46,  1.99it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 84/1250 [01:02<09:46,  1.99it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 85/1250 [01:03<09:40,  2.01it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 86/1250 [01:04<09:58,  1.94it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 87/1250 [01:04<09:48,  1.98it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 88/1250 [01:04<09:42,  2.00it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 89/1250 [01:05<09:38,  2.01it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 90/1250 [01:05<09:34,  2.02it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 91/1250 [01:06<09:52,  1.96it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 92/1250 [01:07<09:44,  1.98it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 93/1250 [01:07<09:38,  2.00it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 94/1250 [01:07<09:31,  2.02it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 95/1250 [01:08<09:57,  1.93it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 96/1250 [01:09<09:47,  1.96it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 97/1250 [01:09<09:39,  1.99it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 98/1250 [01:10<09:35,  2.00it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 99/1250 [01:10<09:30,  2.02it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 100/1250 [01:11<09:48,  1.96it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 101/1250 [01:11<09:40,  1.98it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 102/1250 [01:12<09:35,  1.99it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 103/1250 [01:12<09:31,  2.01it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 104/1250 [01:13<09:34,  2.00it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 105/1250 [01:13<09:51,  1.94it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 106/1250 [01:14<09:37,  1.98it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 107/1250 [01:14<09:30,  2.00it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 108/1250 [01:15<09:29,  2.01it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 109/1250 [01:15<09:34,  1.99it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 110/1250 [01:16<09:58,  1.90it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 111/1250 [01:16<09:47,  1.94it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 112/1250 [01:17<09:42,  1.95it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 113/1250 [01:17<09:34,  1.98it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 114/1250 [01:18<09:58,  1.90it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 115/1250 [01:18<09:45,  1.94it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 116/1250 [01:19<09:35,  1.97it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 117/1250 [01:19<09:27,  2.00it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 118/1250 [01:20<09:21,  2.01it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 119/1250 [01:20<09:41,  1.95it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 120/1250 [01:21<09:32,  1.98it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 121/1250 [01:21<09:24,  2.00it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 122/1250 [01:22<09:19,  2.01it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 123/1250 [01:22<09:17,  2.02it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 124/1250 [01:23<09:35,  1.96it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 125/1250 [01:23<09:27,  1.98it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 2000\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34mNum examples = 2000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/8 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 2/8 [00:00<00:01,  5.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 3/8 [00:00<00:01,  4.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 4/8 [00:01<00:01,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 5/8 [00:01<00:00,  3.02it/s]#033[A\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m75%|███████▌  | 6/8 [00:01<00:00,  2.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 7/8 [00:02<00:00,  2.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  2.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.043243408203125, 'eval_accuracy': 0.6115, 'eval_f1': 0.515432386199899, 'eval_precision': 0.5965313390196564, 'eval_recall': 0.6115, 'eval_runtime': 2.9275, 'eval_samples_per_second': 683.175, 'eval_steps_per_second': 2.733, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m10%|█         | 125/1250 [01:26<09:27,  1.98it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 8/8 [00:02<00:00,  2.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-125\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-125\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-125/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-125/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-125/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-125/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-125/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-125/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-125/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-125/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m10%|█         | 126/1250 [01:30<42:30,  2.27s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 127/1250 [01:30<32:27,  1.73s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 128/1250 [01:31<25:24,  1.36s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 129/1250 [01:31<20:29,  1.10s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 130/1250 [01:32<17:04,  1.09it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 131/1250 [01:32<15:14,  1.22it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 132/1250 [01:33<13:22,  1.39it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 133/1250 [01:33<12:16,  1.52it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 134/1250 [01:34<11:16,  1.65it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 135/1250 [01:34<11:00,  1.69it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 136/1250 [01:35<10:29,  1.77it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 137/1250 [01:35<10:01,  1.85it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 138/1250 [01:36<09:42,  1.91it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 139/1250 [01:36<09:28,  1.95it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 140/1250 [01:37<09:45,  1.89it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 141/1250 [01:37<09:30,  1.94it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 142/1250 [01:38<09:19,  1.98it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 143/1250 [01:38<09:15,  1.99it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 144/1250 [01:39<09:08,  2.02it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 145/1250 [01:39<09:42,  1.90it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 146/1250 [01:40<09:29,  1.94it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 147/1250 [01:40<09:18,  1.98it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 148/1250 [01:41<09:09,  2.00it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 149/1250 [01:41<09:04,  2.02it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 150/1250 [01:42<09:37,  1.90it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 151/1250 [01:42<09:25,  1.94it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 152/1250 [01:43<09:15,  1.98it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 153/1250 [01:43<09:10,  1.99it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 154/1250 [01:44<09:35,  1.90it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 155/1250 [01:44<09:22,  1.95it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 156/1250 [01:45<09:12,  1.98it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 157/1250 [01:45<09:06,  2.00it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 158/1250 [01:46<09:00,  2.02it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 159/1250 [01:46<09:31,  1.91it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 160/1250 [01:47<09:25,  1.93it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 161/1250 [01:47<09:14,  1.96it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 162/1250 [01:48<09:05,  1.99it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 163/1250 [01:48<09:01,  2.01it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 164/1250 [01:49<09:22,  1.93it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 165/1250 [01:49<09:11,  1.97it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 166/1250 [01:50<09:03,  1.99it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 167/1250 [01:50<08:58,  2.01it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 168/1250 [01:51<08:52,  2.03it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 169/1250 [01:51<09:24,  1.91it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 170/1250 [01:52<09:22,  1.92it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 171/1250 [01:52<09:09,  1.96it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 172/1250 [01:53<09:00,  1.99it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 173/1250 [01:53<09:19,  1.92it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 174/1250 [01:54<09:07,  1.97it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 175/1250 [01:54<08:56,  2.00it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 176/1250 [01:55<08:49,  2.03it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 177/1250 [01:55<08:44,  2.04it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 178/1250 [01:56<09:08,  1.95it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 179/1250 [01:56<08:57,  1.99it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 180/1250 [01:57<09:00,  1.98it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 181/1250 [01:57<08:51,  2.01it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 182/1250 [01:58<08:44,  2.04it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 183/1250 [01:58<09:02,  1.97it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 184/1250 [01:59<08:55,  1.99it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 185/1250 [01:59<08:47,  2.02it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 186/1250 [02:00<08:43,  2.03it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 187/1250 [02:00<08:38,  2.05it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 188/1250 [02:01<08:57,  1.98it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 189/1250 [02:01<08:53,  1.99it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 190/1250 [02:02<08:44,  2.02it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 191/1250 [02:02<08:41,  2.03it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 192/1250 [02:03<09:02,  1.95it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 193/1250 [02:03<08:54,  1.98it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 194/1250 [02:04<08:47,  2.00it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 195/1250 [02:04<08:41,  2.02it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 196/1250 [02:05<08:38,  2.03it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 197/1250 [02:05<08:55,  1.97it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 198/1250 [02:06<08:47,  2.00it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 199/1250 [02:06<08:42,  2.01it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 200/1250 [02:07<08:38,  2.02it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 201/1250 [02:07<08:35,  2.03it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 202/1250 [02:08<08:50,  1.97it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 203/1250 [02:08<08:43,  2.00it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 204/1250 [02:09<08:37,  2.02it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 205/1250 [02:09<08:33,  2.03it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 206/1250 [02:10<08:29,  2.05it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 207/1250 [02:10<08:45,  1.99it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 208/1250 [02:11<08:38,  2.01it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 209/1250 [02:11<08:34,  2.02it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 210/1250 [02:12<08:31,  2.03it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 211/1250 [02:12<08:53,  1.95it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 212/1250 [02:13<08:51,  1.95it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 213/1250 [02:13<08:50,  1.95it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 214/1250 [02:14<08:49,  1.96it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 215/1250 [02:14<08:40,  1.99it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 216/1250 [02:15<08:50,  1.95it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 217/1250 [02:15<08:50,  1.95it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 218/1250 [02:16<08:44,  1.97it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 219/1250 [02:16<08:39,  1.99it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 220/1250 [02:17<08:32,  2.01it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 221/1250 [02:17<08:50,  1.94it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 222/1250 [02:18<08:41,  1.97it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 223/1250 [02:18<08:33,  2.00it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 224/1250 [02:19<08:28,  2.02it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 225/1250 [02:19<08:24,  2.03it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 226/1250 [02:20<08:47,  1.94it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 227/1250 [02:20<08:37,  1.98it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 228/1250 [02:21<08:30,  2.00it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 229/1250 [02:21<08:25,  2.02it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 230/1250 [02:22<08:44,  1.95it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 231/1250 [02:23<08:34,  1.98it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 232/1250 [02:23<08:27,  2.01it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 233/1250 [02:23<08:23,  2.02it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 234/1250 [02:24<08:29,  2.00it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 235/1250 [02:25<08:44,  1.93it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 236/1250 [02:25<08:34,  1.97it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m19%|█▉        | 237/1250 [02:26<08:26,  2.00it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 238/1250 [02:26<08:21,  2.02it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 239/1250 [02:26<08:19,  2.03it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 240/1250 [02:27<08:32,  1.97it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 241/1250 [02:28<08:24,  2.00it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 242/1250 [02:28<08:19,  2.02it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 243/1250 [02:28<08:13,  2.04it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 244/1250 [02:29<08:34,  1.96it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 245/1250 [02:30<08:25,  1.99it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 246/1250 [02:30<08:19,  2.01it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 247/1250 [02:30<08:15,  2.03it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 248/1250 [02:31<08:11,  2.04it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 249/1250 [02:32<08:33,  1.95it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 250/1250 [02:32<08:23,  1.99it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/8 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 2/8 [00:00<00:01,  5.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 3/8 [00:00<00:01,  4.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 4/8 [00:01<00:01,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 5/8 [00:01<00:00,  3.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 6/8 [00:01<00:00,  3.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 7/8 [00:02<00:00,  3.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  3.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.29452672600746155, 'eval_accuracy': 0.9015, 'eval_f1': 0.9028380594099472, 'eval_precision': 0.9081824676999165, 'eval_recall': 0.9015, 'eval_runtime': 2.8064, 'eval_samples_per_second': 712.668, 'eval_steps_per_second': 2.851, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m20%|██        | 250/1250 [02:35<08:23,  1.99it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-250\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-250\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-250/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-250/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-250/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-250/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-250/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-250/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-250/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-250/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-125] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-125] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m20%|██        | 251/1250 [02:38<37:46,  2.27s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 252/1250 [02:39<28:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 253/1250 [02:39<22:35,  1.36s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 254/1250 [02:40<18:12,  1.10s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 255/1250 [02:40<15:08,  1.09it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 256/1250 [02:41<13:18,  1.24it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 257/1250 [02:41<11:43,  1.41it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 258/1250 [02:42<10:37,  1.56it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 259/1250 [02:42<09:49,  1.68it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 260/1250 [02:43<09:16,  1.78it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 261/1250 [02:43<09:08,  1.80it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 262/1250 [02:44<08:46,  1.88it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 263/1250 [02:44<08:31,  1.93it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 264/1250 [02:45<08:20,  1.97it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 265/1250 [02:45<08:13,  2.00it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 266/1250 [02:46<08:23,  1.95it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 267/1250 [02:46<08:16,  1.98it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 268/1250 [02:47<08:08,  2.01it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 269/1250 [02:47<08:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 270/1250 [02:48<08:25,  1.94it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 271/1250 [02:48<08:15,  1.98it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 272/1250 [02:49<08:07,  2.01it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 273/1250 [02:49<08:03,  2.02it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 274/1250 [02:50<08:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 275/1250 [02:50<08:15,  1.97it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 276/1250 [02:51<08:07,  2.00it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 277/1250 [02:51<08:01,  2.02it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 278/1250 [02:52<07:59,  2.03it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 279/1250 [02:52<07:56,  2.04it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 280/1250 [02:53<08:09,  1.98it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 281/1250 [02:53<08:03,  2.01it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 282/1250 [02:54<07:57,  2.03it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 283/1250 [02:54<07:55,  2.03it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 284/1250 [02:55<07:52,  2.05it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 285/1250 [02:55<08:07,  1.98it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 286/1250 [02:56<08:01,  2.00it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 287/1250 [02:56<07:56,  2.02it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 288/1250 [02:57<07:53,  2.03it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 289/1250 [02:57<08:15,  1.94it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 290/1250 [02:58<08:07,  1.97it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 291/1250 [02:58<08:01,  1.99it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 292/1250 [02:59<07:55,  2.02it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 293/1250 [02:59<07:52,  2.03it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 294/1250 [03:00<08:05,  1.97it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 295/1250 [03:00<07:58,  2.00it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 296/1250 [03:01<07:52,  2.02it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 297/1250 [03:01<07:49,  2.03it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 298/1250 [03:02<07:48,  2.03it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 299/1250 [03:02<08:01,  1.97it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 300/1250 [03:03<07:55,  2.00it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 301/1250 [03:03<07:49,  2.02it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 302/1250 [03:04<07:51,  2.01it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 303/1250 [03:04<07:47,  2.03it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 304/1250 [03:05<08:00,  1.97it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 305/1250 [03:05<07:53,  2.00it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 306/1250 [03:06<07:48,  2.01it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 307/1250 [03:06<07:44,  2.03it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 308/1250 [03:07<08:03,  1.95it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 309/1250 [03:07<07:55,  1.98it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 310/1250 [03:08<07:49,  2.00it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 311/1250 [03:08<07:44,  2.02it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 312/1250 [03:09<07:46,  2.01it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 313/1250 [03:09<08:00,  1.95it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 314/1250 [03:10<08:00,  1.95it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 315/1250 [03:10<07:53,  1.98it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 316/1250 [03:11<07:48,  1.99it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 317/1250 [03:11<07:45,  2.00it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 318/1250 [03:12<07:59,  1.94it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 319/1250 [03:12<07:50,  1.98it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 320/1250 [03:13<07:47,  1.99it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 321/1250 [03:13<07:43,  2.01it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 322/1250 [03:14<08:01,  1.93it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 323/1250 [03:14<07:59,  1.93it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 324/1250 [03:15<07:52,  1.96it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 325/1250 [03:15<07:50,  1.97it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 326/1250 [03:16<07:48,  1.97it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 327/1250 [03:16<07:57,  1.93it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 328/1250 [03:17<07:44,  1.98it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 329/1250 [03:17<07:45,  1.98it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 330/1250 [03:18<07:40,  2.00it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 331/1250 [03:18<07:35,  2.02it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 332/1250 [03:19<07:48,  1.96it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 333/1250 [03:19<07:40,  1.99it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 334/1250 [03:20<07:41,  1.98it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 335/1250 [03:20<07:37,  2.00it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 336/1250 [03:21<07:33,  2.01it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 337/1250 [03:21<07:48,  1.95it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 338/1250 [03:22<07:38,  1.99it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 339/1250 [03:22<07:34,  2.01it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 340/1250 [03:23<07:31,  2.02it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 341/1250 [03:23<07:44,  1.96it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 342/1250 [03:24<07:40,  1.97it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 343/1250 [03:24<07:35,  1.99it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 344/1250 [03:25<07:31,  2.00it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 345/1250 [03:25<07:29,  2.01it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 346/1250 [03:26<07:51,  1.92it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 347/1250 [03:27<07:42,  1.95it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 348/1250 [03:27<07:36,  1.98it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m28%|██▊       | 349/1250 [03:28<07:30,  2.00it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 350/1250 [03:28<07:27,  2.01it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 351/1250 [03:29<07:40,  1.95it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 352/1250 [03:29<07:32,  1.98it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 353/1250 [03:30<07:29,  2.00it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 354/1250 [03:30<07:27,  2.00it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 355/1250 [03:31<07:24,  2.02it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 356/1250 [03:31<07:38,  1.95it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 357/1250 [03:32<07:31,  1.98it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 358/1250 [03:32<07:26,  2.00it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 359/1250 [03:33<07:23,  2.01it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 360/1250 [03:33<07:40,  1.93it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 361/1250 [03:34<07:32,  1.96it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 362/1250 [03:34<07:25,  1.99it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 363/1250 [03:35<07:20,  2.02it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 364/1250 [03:35<07:18,  2.02it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 365/1250 [03:36<07:37,  1.94it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 366/1250 [03:36<07:28,  1.97it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 367/1250 [03:37<07:22,  2.00it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 368/1250 [03:37<07:17,  2.01it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 369/1250 [03:38<07:15,  2.02it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 370/1250 [03:38<07:29,  1.96it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 371/1250 [03:39<07:23,  1.98it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 372/1250 [03:39<07:22,  1.98it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 373/1250 [03:40<07:17,  2.01it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 374/1250 [03:40<07:15,  2.01it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 375/1250 [03:41<07:27,  1.95it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/8 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 2/8 [00:00<00:01,  5.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 3/8 [00:00<00:01,  4.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 4/8 [00:01<00:01,  3.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 5/8 [00:01<00:00,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 6/8 [00:01<00:00,  3.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  3.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.18347296118736267, 'eval_accuracy': 0.9275, 'eval_f1': 0.9283537917442808, 'eval_precision': 0.9317144101857665, 'eval_recall': 0.9275, 'eval_runtime': 2.8057, 'eval_samples_per_second': 712.831, 'eval_steps_per_second': 2.851, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m30%|███       | 375/1250 [03:43<07:27,  1.95it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  3.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-375\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-375\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-375/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-375/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-375/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-375/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-375/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-375/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-375/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-375/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-250] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-250] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m30%|███       | 376/1250 [03:47<33:07,  2.27s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 377/1250 [03:48<25:34,  1.76s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 378/1250 [03:48<20:01,  1.38s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 379/1250 [03:49<16:09,  1.11s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 380/1250 [03:49<13:25,  1.08it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 381/1250 [03:50<11:47,  1.23it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 382/1250 [03:50<10:22,  1.40it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 383/1250 [03:51<09:21,  1.54it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 384/1250 [03:51<08:39,  1.67it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 385/1250 [03:52<08:11,  1.76it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 386/1250 [03:52<08:11,  1.76it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 387/1250 [03:53<07:50,  1.83it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 388/1250 [03:53<07:36,  1.89it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 389/1250 [03:54<07:26,  1.93it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 390/1250 [03:54<07:18,  1.96it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 391/1250 [03:55<07:25,  1.93it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 392/1250 [03:55<07:17,  1.96it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 393/1250 [03:56<07:08,  2.00it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 394/1250 [03:56<07:03,  2.02it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 395/1250 [03:57<07:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 396/1250 [03:57<07:12,  1.97it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 397/1250 [03:58<07:06,  2.00it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 398/1250 [03:58<07:02,  2.02it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 399/1250 [03:59<06:58,  2.03it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 400/1250 [03:59<07:16,  1.95it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 401/1250 [04:00<07:06,  1.99it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 402/1250 [04:00<07:00,  2.01it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 403/1250 [04:01<06:58,  2.02it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 404/1250 [04:01<06:56,  2.03it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 405/1250 [04:02<07:16,  1.94it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 406/1250 [04:02<07:09,  1.96it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 407/1250 [04:03<07:04,  1.99it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 408/1250 [04:03<06:59,  2.01it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 409/1250 [04:04<06:55,  2.02it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 410/1250 [04:04<07:06,  1.97it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 411/1250 [04:05<07:00,  2.00it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 412/1250 [04:05<06:55,  2.02it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 413/1250 [04:06<06:52,  2.03it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 414/1250 [04:06<06:49,  2.04it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 415/1250 [04:07<07:01,  1.98it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 416/1250 [04:07<06:56,  2.00it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 417/1250 [04:08<06:51,  2.02it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 418/1250 [04:08<06:49,  2.03it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 419/1250 [04:09<07:07,  1.94it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 420/1250 [04:09<06:59,  1.98it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 421/1250 [04:10<06:53,  2.01it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 422/1250 [04:10<06:49,  2.02it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 423/1250 [04:11<06:47,  2.03it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 424/1250 [04:11<07:06,  1.94it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 425/1250 [04:12<06:57,  1.97it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 426/1250 [04:12<06:53,  1.99it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 427/1250 [04:13<06:49,  2.01it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 428/1250 [04:13<06:51,  2.00it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 429/1250 [04:14<07:00,  1.95it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 430/1250 [04:14<07:01,  1.95it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 431/1250 [04:15<06:53,  1.98it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 432/1250 [04:15<06:49,  2.00it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 433/1250 [04:16<06:50,  1.99it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 434/1250 [04:16<07:03,  1.93it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 435/1250 [04:17<06:59,  1.94it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 436/1250 [04:17<06:58,  1.94it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 437/1250 [04:18<06:52,  1.97it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 438/1250 [04:18<07:04,  1.91it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 439/1250 [04:19<06:55,  1.95it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 440/1250 [04:19<06:48,  1.98it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 441/1250 [04:20<06:43,  2.00it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 442/1250 [04:20<06:39,  2.02it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 443/1250 [04:21<06:55,  1.94it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 444/1250 [04:21<06:47,  1.98it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 445/1250 [04:22<06:42,  2.00it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 446/1250 [04:22<06:38,  2.02it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 447/1250 [04:23<06:36,  2.03it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 448/1250 [04:23<06:47,  1.97it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 449/1250 [04:24<06:41,  2.00it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 450/1250 [04:24<06:38,  2.01it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 451/1250 [04:25<06:35,  2.02it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 452/1250 [04:25<06:32,  2.03it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 453/1250 [04:26<06:44,  1.97it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 454/1250 [04:26<06:38,  2.00it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 455/1250 [04:27<06:33,  2.02it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 456/1250 [04:27<06:31,  2.03it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 457/1250 [04:28<06:47,  1.94it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 458/1250 [04:28<06:39,  1.98it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m37%|███▋      | 459/1250 [04:29<06:34,  2.00it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 460/1250 [04:29<06:31,  2.02it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 461/1250 [04:30<06:29,  2.02it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 462/1250 [04:30<06:42,  1.96it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 463/1250 [04:31<06:36,  1.98it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 464/1250 [04:31<06:31,  2.01it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 465/1250 [04:32<06:30,  2.01it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 466/1250 [04:32<06:29,  2.01it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 467/1250 [04:33<06:38,  1.96it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 468/1250 [04:33<06:33,  1.99it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 469/1250 [04:34<06:28,  2.01it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 470/1250 [04:34<06:25,  2.02it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 471/1250 [04:35<06:22,  2.04it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 472/1250 [04:35<06:33,  1.98it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 473/1250 [04:36<06:29,  1.99it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 474/1250 [04:36<06:25,  2.01it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 475/1250 [04:37<06:23,  2.02it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 476/1250 [04:37<06:38,  1.94it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 477/1250 [04:38<06:31,  1.98it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 478/1250 [04:38<06:26,  2.00it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 479/1250 [04:39<06:22,  2.01it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 480/1250 [04:39<06:24,  2.00it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 481/1250 [04:40<06:39,  1.93it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 482/1250 [04:40<06:30,  1.97it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 483/1250 [04:41<06:25,  1.99it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 484/1250 [04:41<06:21,  2.01it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 485/1250 [04:42<06:18,  2.02it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 486/1250 [04:42<06:28,  1.96it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 487/1250 [04:43<06:23,  1.99it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 488/1250 [04:43<06:18,  2.01it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 489/1250 [04:44<06:15,  2.02it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 490/1250 [04:44<06:14,  2.03it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 491/1250 [04:45<06:24,  1.97it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 492/1250 [04:45<06:19,  2.00it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 493/1250 [04:46<06:14,  2.02it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 494/1250 [04:46<06:13,  2.03it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 495/1250 [04:47<06:29,  1.94it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 496/1250 [04:47<06:21,  1.97it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 497/1250 [04:48<06:17,  2.00it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 498/1250 [04:48<06:12,  2.02it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 499/1250 [04:49<06:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 500/1250 [04:49<06:21,  1.96it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6634, 'learning_rate': 3e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m40%|████      | 500/1250 [04:49<06:21,  1.96it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/8 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 2/8 [00:00<00:01,  5.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 3/8 [00:00<00:01,  4.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 4/8 [00:01<00:01,  3.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 5/8 [00:01<00:00,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 6/8 [00:01<00:00,  3.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  3.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1570882946252823, 'eval_accuracy': 0.9375, 'eval_f1': 0.9371352268173966, 'eval_precision': 0.9388617115148703, 'eval_recall': 0.9375, 'eval_runtime': 2.7848, 'eval_samples_per_second': 718.189, 'eval_steps_per_second': 2.873, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m40%|████      | 500/1250 [04:52<06:21,  1.96it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 8/8 [00:02<00:00,  3.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-375] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-375] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m40%|████      | 501/1250 [04:56<28:16,  2.26s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 502/1250 [04:56<21:47,  1.75s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 503/1250 [04:57<17:03,  1.37s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 504/1250 [04:57<13:43,  1.10s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 505/1250 [04:58<11:24,  1.09it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 506/1250 [04:58<09:47,  1.27it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 507/1250 [04:59<08:50,  1.40it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 508/1250 [04:59<08:00,  1.54it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 509/1250 [05:00<07:24,  1.67it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 510/1250 [05:00<06:58,  1.77it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 511/1250 [05:01<06:40,  1.85it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 512/1250 [05:01<06:40,  1.84it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 513/1250 [05:02<06:27,  1.90it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 514/1250 [05:02<06:18,  1.95it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 515/1250 [05:03<06:11,  1.98it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 516/1250 [05:03<06:23,  1.91it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 517/1250 [05:04<06:14,  1.96it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 518/1250 [05:04<06:08,  1.99it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 519/1250 [05:05<06:04,  2.01it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 520/1250 [05:05<06:00,  2.02it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 521/1250 [05:06<06:10,  1.97it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 522/1250 [05:06<06:05,  1.99it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 523/1250 [05:07<06:01,  2.01it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 524/1250 [05:07<05:57,  2.03it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 525/1250 [05:08<05:54,  2.04it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 526/1250 [05:08<06:05,  1.98it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 527/1250 [05:09<06:00,  2.01it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 528/1250 [05:09<05:56,  2.03it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 529/1250 [05:10<05:53,  2.04it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 530/1250 [05:10<05:50,  2.05it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 531/1250 [05:11<06:01,  1.99it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 532/1250 [05:11<05:57,  2.01it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 533/1250 [05:12<05:53,  2.03it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 534/1250 [05:12<05:51,  2.04it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 535/1250 [05:13<06:08,  1.94it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 536/1250 [05:13<06:02,  1.97it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 537/1250 [05:14<05:58,  1.99it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 538/1250 [05:14<05:55,  2.01it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 539/1250 [05:15<05:54,  2.01it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 540/1250 [05:15<06:13,  1.90it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 541/1250 [05:16<06:10,  1.91it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 542/1250 [05:16<06:05,  1.94it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 543/1250 [05:17<06:03,  1.94it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\u001b[0m\n",
      "\u001b[34m44%|████▎     | 544/1250 [05:17<06:00,  1.96it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 545/1250 [05:18<06:16,  1.87it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 546/1250 [05:18<06:06,  1.92it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 547/1250 [05:19<06:04,  1.93it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 548/1250 [05:19<06:03,  1.93it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 549/1250 [05:20<05:55,  1.97it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 550/1250 [05:20<06:07,  1.90it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 551/1250 [05:21<05:58,  1.95it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 552/1250 [05:21<05:56,  1.96it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 553/1250 [05:22<05:56,  1.95it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 554/1250 [05:22<06:04,  1.91it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 555/1250 [05:23<05:56,  1.95it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 556/1250 [05:23<05:52,  1.97it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 557/1250 [05:24<05:46,  2.00it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m45%|████▍     | 558/1250 [05:24<05:45,  2.01it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 559/1250 [05:25<05:59,  1.92it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 560/1250 [05:26<05:57,  1.93it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 561/1250 [05:26<05:50,  1.96it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 562/1250 [05:27<05:52,  1.95it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 563/1250 [05:27<05:46,  1.98it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 564/1250 [05:28<06:02,  1.89it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 565/1250 [05:28<05:55,  1.93it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 566/1250 [05:29<05:49,  1.96it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 567/1250 [05:29<05:43,  1.99it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 568/1250 [05:30<05:41,  2.00it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 569/1250 [05:30<05:58,  1.90it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 570/1250 [05:31<05:52,  1.93it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 571/1250 [05:31<05:46,  1.96it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 572/1250 [05:32<05:40,  1.99it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 573/1250 [05:32<05:50,  1.93it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 574/1250 [05:33<05:44,  1.96it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 575/1250 [05:33<05:36,  2.01it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 576/1250 [05:34<05:35,  2.01it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 577/1250 [05:34<05:36,  2.00it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 578/1250 [05:35<05:50,  1.92it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 579/1250 [05:35<05:44,  1.95it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 580/1250 [05:36<05:38,  1.98it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 581/1250 [05:36<05:35,  1.99it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 582/1250 [05:37<05:33,  2.00it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 583/1250 [05:37<05:46,  1.92it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 584/1250 [05:38<05:44,  1.93it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 585/1250 [05:38<05:37,  1.97it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 586/1250 [05:39<05:33,  1.99it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 587/1250 [05:39<05:48,  1.90it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 588/1250 [05:40<05:42,  1.94it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 589/1250 [05:40<05:36,  1.96it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 590/1250 [05:41<05:36,  1.96it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 591/1250 [05:41<05:31,  1.99it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 592/1250 [05:42<05:44,  1.91it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 593/1250 [05:42<05:37,  1.95it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 594/1250 [05:43<05:32,  1.98it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 595/1250 [05:43<05:27,  2.00it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 596/1250 [05:44<05:25,  2.01it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 597/1250 [05:44<05:37,  1.93it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 598/1250 [05:45<05:30,  1.98it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 599/1250 [05:45<05:26,  1.99it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 600/1250 [05:46<05:28,  1.98it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 601/1250 [05:46<05:30,  1.97it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 602/1250 [05:47<05:43,  1.89it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 603/1250 [05:47<05:35,  1.93it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 604/1250 [05:48<05:28,  1.97it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 605/1250 [05:48<05:22,  2.00it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 606/1250 [05:49<05:40,  1.89it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 607/1250 [05:50<05:32,  1.93it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 608/1250 [05:50<05:27,  1.96it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 609/1250 [05:50<05:24,  1.98it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 610/1250 [05:51<05:19,  2.00it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 611/1250 [05:52<05:28,  1.94it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 612/1250 [05:52<05:27,  1.95it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 613/1250 [05:53<05:21,  1.98it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 614/1250 [05:53<05:17,  2.00it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 615/1250 [05:54<05:15,  2.01it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 616/1250 [05:54<05:24,  1.95it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 617/1250 [05:55<05:19,  1.98it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 618/1250 [05:55<05:15,  2.00it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 619/1250 [05:56<05:17,  1.99it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 620/1250 [05:56<05:13,  2.01it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 621/1250 [05:57<05:21,  1.96it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 622/1250 [05:57<05:21,  1.95it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 623/1250 [05:58<05:19,  1.96it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 624/1250 [05:58<05:14,  1.99it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 625/1250 [05:59<05:23,  1.93it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/8 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 2/8 [00:00<00:01,  5.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 3/8 [00:00<00:01,  4.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 4/8 [00:01<00:01,  3.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 5/8 [00:01<00:00,  3.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 6/8 [00:01<00:00,  2.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 7/8 [00:02<00:00,  2.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  2.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.14944027364253998, 'eval_accuracy': 0.9365, 'eval_f1': 0.9365083964115908, 'eval_precision': 0.9389701980772266, 'eval_recall': 0.9365, 'eval_runtime': 2.804, 'eval_samples_per_second': 713.262, 'eval_steps_per_second': 2.853, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m50%|█████     | 625/1250 [06:01<05:23,  1.93it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 8/8 [00:02<00:00,  2.98it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-625\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-625\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-625/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-625/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-625/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-625/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-625/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-625/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-625/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-625/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m50%|█████     | 626/1250 [06:05<22:54,  2.20s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 627/1250 [06:05<17:37,  1.70s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 628/1250 [06:06<14:01,  1.35s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 629/1250 [06:06<11:19,  1.09s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 630/1250 [06:07<09:26,  1.09it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 631/1250 [06:07<08:04,  1.28it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 632/1250 [06:08<07:29,  1.38it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 633/1250 [06:08<06:45,  1.52it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 634/1250 [06:09<06:20,  1.62it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 635/1250 [06:09<05:58,  1.71it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 636/1250 [06:10<05:38,  1.81it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 637/1250 [06:10<05:41,  1.80it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 638/1250 [06:11<05:32,  1.84it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 639/1250 [06:11<05:20,  1.90it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 640/1250 [06:12<05:13,  1.95it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 641/1250 [06:12<05:08,  1.98it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 642/1250 [06:13<05:17,  1.91it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 643/1250 [06:13<05:12,  1.94it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 644/1250 [06:14<05:12,  1.94it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 645/1250 [06:15<05:11,  1.94it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 646/1250 [06:15<05:05,  1.98it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 647/1250 [06:16<05:16,  1.91it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 648/1250 [06:16<05:08,  1.95it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 649/1250 [06:17<05:05,  1.96it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 650/1250 [06:17<05:01,  1.99it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 651/1250 [06:18<05:12,  1.92it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 652/1250 [06:18<05:06,  1.95it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 653/1250 [06:19<05:00,  1.98it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 654/1250 [06:19<04:58,  2.00it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 655/1250 [06:20<04:56,  2.01it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 656/1250 [06:20<05:04,  1.95it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m53%|█████▎    | 657/1250 [06:21<04:59,  1.98it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 658/1250 [06:21<05:00,  1.97it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 659/1250 [06:22<04:56,  1.99it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 660/1250 [06:22<04:53,  2.01it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 661/1250 [06:23<05:00,  1.96it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 662/1250 [06:23<04:55,  1.99it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 663/1250 [06:24<04:52,  2.00it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 664/1250 [06:24<04:50,  2.02it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 665/1250 [06:25<05:01,  1.94it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 666/1250 [06:25<05:02,  1.93it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 667/1250 [06:26<04:55,  1.97it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 668/1250 [06:26<04:51,  2.00it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 669/1250 [06:27<04:54,  1.97it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 670/1250 [06:27<05:03,  1.91it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 671/1250 [06:28<04:56,  1.95it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 672/1250 [06:28<04:51,  1.99it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 673/1250 [06:29<04:48,  2.00it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 674/1250 [06:29<04:46,  2.01it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 675/1250 [06:30<04:54,  1.95it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 676/1250 [06:30<04:50,  1.98it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 677/1250 [06:31<04:47,  1.99it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 678/1250 [06:31<04:47,  1.99it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 679/1250 [06:32<04:45,  2.00it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 680/1250 [06:32<04:52,  1.95it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 681/1250 [06:33<04:51,  1.95it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 682/1250 [06:33<04:46,  1.98it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 683/1250 [06:34<04:43,  2.00it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 684/1250 [06:34<04:55,  1.92it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 685/1250 [06:35<04:50,  1.94it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 686/1250 [06:35<04:45,  1.98it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 687/1250 [06:36<04:41,  2.00it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 688/1250 [06:36<04:38,  2.02it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 689/1250 [06:37<04:50,  1.93it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 690/1250 [06:37<04:48,  1.94it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 691/1250 [06:38<04:48,  1.93it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 692/1250 [06:38<04:44,  1.96it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 693/1250 [06:39<04:40,  1.99it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 694/1250 [06:39<04:49,  1.92it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 695/1250 [06:40<04:48,  1.92it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 696/1250 [06:40<04:44,  1.94it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 697/1250 [06:41<04:39,  1.98it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 698/1250 [06:41<04:38,  1.98it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 699/1250 [06:42<04:48,  1.91it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 700/1250 [06:42<04:45,  1.93it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 701/1250 [06:43<04:41,  1.95it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 702/1250 [06:43<04:41,  1.95it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 703/1250 [06:44<04:51,  1.87it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 704/1250 [06:45<04:46,  1.91it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 705/1250 [06:45<04:39,  1.95it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 706/1250 [06:46<04:35,  1.98it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 707/1250 [06:46<04:31,  2.00it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 708/1250 [06:47<04:37,  1.95it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 709/1250 [06:47<04:32,  1.98it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 710/1250 [06:48<04:29,  2.00it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 711/1250 [06:48<04:28,  2.01it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 712/1250 [06:49<04:28,  2.00it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 713/1250 [06:49<04:38,  1.93it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 714/1250 [06:50<04:32,  1.97it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 715/1250 [06:50<04:29,  1.99it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 716/1250 [06:51<04:29,  1.98it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 717/1250 [06:51<04:31,  1.96it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 718/1250 [06:52<04:40,  1.90it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 719/1250 [06:52<04:32,  1.95it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 720/1250 [06:53<04:26,  1.99it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 721/1250 [06:53<04:25,  1.99it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 722/1250 [06:54<04:36,  1.91it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 723/1250 [06:54<04:29,  1.95it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 724/1250 [06:55<04:30,  1.95it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 725/1250 [06:55<04:23,  1.99it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 726/1250 [06:56<04:18,  2.03it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 727/1250 [06:56<04:29,  1.94it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 728/1250 [06:57<04:29,  1.94it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 729/1250 [06:57<04:23,  1.98it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 730/1250 [06:58<04:18,  2.01it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 731/1250 [06:58<04:20,  2.00it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 732/1250 [06:59<04:30,  1.92it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 733/1250 [06:59<04:28,  1.92it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 734/1250 [07:00<04:26,  1.94it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 735/1250 [07:00<04:25,  1.94it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 736/1250 [07:01<04:24,  1.95it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 737/1250 [07:01<04:31,  1.89it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 738/1250 [07:02<04:28,  1.90it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 739/1250 [07:02<04:26,  1.92it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 740/1250 [07:03<04:25,  1.92it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 741/1250 [07:04<04:32,  1.87it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 742/1250 [07:04<04:28,  1.89it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 743/1250 [07:05<04:20,  1.95it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 744/1250 [07:05<04:20,  1.94it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 745/1250 [07:06<04:19,  1.94it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 746/1250 [07:06<04:29,  1.87it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 747/1250 [07:07<04:21,  1.92it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 748/1250 [07:07<04:19,  1.93it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 749/1250 [07:08<04:13,  1.97it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 750/1250 [07:08<04:14,  1.96it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/8 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 2/8 [00:00<00:01,  5.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 3/8 [00:00<00:01,  4.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 4/8 [00:01<00:01,  3.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 5/8 [00:01<00:00,  3.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 6/8 [00:01<00:00,  2.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 7/8 [00:02<00:00,  2.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  3.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1413620561361313, 'eval_accuracy': 0.9385, 'eval_f1': 0.9389094451825101, 'eval_precision': 0.9398402568447541, 'eval_recall': 0.9385, 'eval_runtime': 2.8657, 'eval_samples_per_second': 697.899, 'eval_steps_per_second': 2.792, 'epoch': 6.0}\u001b[0m\n",
      "\u001b[34m60%|██████    | 750/1250 [07:11<04:14,  1.96it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 8/8 [00:02<00:00,  3.00it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-750\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-750\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-750/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-750/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-750/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-750/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-750/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-750/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-750/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-750/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-500] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-500] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-625] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-625] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m60%|██████    | 751/1250 [07:15<19:15,  2.32s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 752/1250 [07:15<14:44,  1.78s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 753/1250 [07:16<11:43,  1.42s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 754/1250 [07:16<09:26,  1.14s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 755/1250 [07:17<07:52,  1.05it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 756/1250 [07:17<06:42,  1.23it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 757/1250 [07:18<05:53,  1.39it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 758/1250 [07:18<05:33,  1.47it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 759/1250 [07:19<05:09,  1.59it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 760/1250 [07:19<04:47,  1.71it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 761/1250 [07:20<04:31,  1.80it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 762/1250 [07:20<04:25,  1.84it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 763/1250 [07:21<04:27,  1.82it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 764/1250 [07:21<04:21,  1.86it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 765/1250 [07:22<04:12,  1.92it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 766/1250 [07:22<04:06,  1.97it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 767/1250 [07:23<04:12,  1.92it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m61%|██████▏   | 768/1250 [07:23<04:05,  1.96it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 769/1250 [07:24<04:01,  1.99it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 770/1250 [07:24<04:01,  1.99it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 771/1250 [07:25<03:57,  2.01it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 772/1250 [07:25<04:06,  1.94it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 773/1250 [07:26<04:01,  1.98it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 774/1250 [07:26<04:01,  1.97it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 775/1250 [07:27<03:57,  2.00it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 776/1250 [07:27<03:58,  1.99it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 777/1250 [07:28<04:07,  1.91it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 778/1250 [07:29<04:05,  1.92it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 779/1250 [07:29<04:03,  1.93it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 780/1250 [07:30<04:02,  1.94it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 781/1250 [07:30<04:11,  1.86it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 782/1250 [07:31<04:04,  1.91it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 783/1250 [07:31<03:59,  1.95it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 784/1250 [07:32<03:56,  1.97it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 785/1250 [07:32<03:52,  2.00it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 786/1250 [07:33<03:59,  1.94it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 787/1250 [07:33<03:54,  1.98it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 788/1250 [07:34<03:51,  1.99it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 789/1250 [07:34<03:49,  2.01it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 790/1250 [07:35<03:47,  2.02it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 791/1250 [07:35<03:56,  1.94it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 792/1250 [07:36<03:52,  1.97it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 793/1250 [07:36<03:51,  1.97it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 794/1250 [07:37<03:52,  1.96it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 795/1250 [07:37<03:47,  2.00it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 796/1250 [07:38<03:54,  1.93it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 797/1250 [07:38<03:54,  1.93it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 798/1250 [07:39<03:50,  1.96it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 799/1250 [07:39<03:50,  1.96it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 800/1250 [07:40<03:57,  1.90it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 801/1250 [07:40<03:51,  1.94it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 802/1250 [07:41<03:49,  1.95it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 803/1250 [07:41<03:48,  1.96it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 804/1250 [07:42<03:47,  1.96it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 805/1250 [07:42<03:57,  1.87it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 806/1250 [07:43<03:51,  1.92it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 807/1250 [07:43<03:48,  1.94it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 808/1250 [07:44<03:46,  1.95it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 809/1250 [07:44<03:43,  1.98it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 810/1250 [07:45<03:50,  1.91it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 811/1250 [07:45<03:45,  1.95it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 812/1250 [07:46<03:49,  1.91it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 813/1250 [07:46<03:48,  1.91it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 814/1250 [07:47<03:44,  1.94it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 815/1250 [07:48<03:51,  1.88it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 816/1250 [07:48<03:46,  1.92it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 817/1250 [07:49<03:42,  1.95it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 818/1250 [07:49<03:39,  1.97it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 819/1250 [07:50<03:45,  1.91it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 820/1250 [07:50<03:40,  1.95it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 821/1250 [07:51<03:39,  1.96it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 822/1250 [07:51<03:35,  1.98it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 823/1250 [07:52<03:34,  1.99it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 824/1250 [07:52<03:43,  1.91it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 825/1250 [07:53<03:38,  1.95it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 826/1250 [07:53<03:38,  1.94it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 827/1250 [07:54<03:34,  1.97it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 828/1250 [07:54<03:31,  1.99it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 829/1250 [07:55<03:40,  1.91it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 830/1250 [07:55<03:35,  1.95it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 831/1250 [07:56<03:32,  1.97it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 832/1250 [07:56<03:29,  2.00it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 833/1250 [07:57<03:28,  2.00it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 834/1250 [07:57<03:36,  1.92it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 835/1250 [07:58<03:32,  1.96it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 836/1250 [07:58<03:30,  1.96it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 837/1250 [07:59<03:28,  1.98it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 838/1250 [07:59<03:34,  1.92it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 839/1250 [08:00<03:33,  1.93it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 840/1250 [08:00<03:29,  1.96it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 841/1250 [08:01<03:29,  1.95it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 842/1250 [08:01<03:26,  1.97it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 843/1250 [08:02<03:34,  1.90it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 844/1250 [08:02<03:29,  1.94it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 845/1250 [08:03<03:26,  1.96it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 846/1250 [08:03<03:23,  1.98it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 847/1250 [08:04<03:21,  2.00it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 848/1250 [08:04<03:28,  1.92it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 849/1250 [08:05<03:24,  1.96it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 850/1250 [08:05<03:21,  1.99it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 851/1250 [08:06<03:19,  2.00it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 852/1250 [08:06<03:18,  2.01it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 853/1250 [08:07<03:25,  1.93it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 854/1250 [08:07<03:21,  1.97it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 855/1250 [08:08<03:18,  1.99it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 856/1250 [08:08<03:16,  2.01it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 857/1250 [08:09<03:22,  1.94it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 858/1250 [08:09<03:21,  1.95it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 859/1250 [08:10<03:20,  1.95it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 860/1250 [08:10<03:19,  1.96it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 861/1250 [08:11<03:16,  1.98it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 862/1250 [08:12<03:24,  1.90it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 863/1250 [08:12<03:19,  1.94it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 864/1250 [08:13<03:17,  1.95it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 865/1250 [08:13<03:18,  1.94it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 866/1250 [08:14<03:16,  1.95it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 867/1250 [08:14<03:25,  1.86it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 868/1250 [08:15<03:18,  1.92it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 869/1250 [08:15<03:15,  1.95it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 870/1250 [08:16<03:13,  1.97it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 871/1250 [08:16<03:11,  1.98it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 872/1250 [08:17<03:19,  1.90it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 873/1250 [08:17<03:14,  1.94it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 874/1250 [08:18<03:11,  1.97it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 875/1250 [08:18<03:09,  1.98it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/8 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 2/8 [00:00<00:01,  5.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 3/8 [00:00<00:01,  4.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 4/8 [00:01<00:01,  3.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 5/8 [00:01<00:00,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 6/8 [00:01<00:00,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 7/8 [00:02<00:00,  2.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  2.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.15445710718631744, 'eval_accuracy': 0.94, 'eval_f1': 0.9390823010532816, 'eval_precision': 0.9399218107094516, 'eval_recall': 0.94, 'eval_runtime': 2.904, 'eval_samples_per_second': 688.7, 'eval_steps_per_second': 2.755, 'epoch': 7.0}\u001b[0m\n",
      "\u001b[34m70%|███████   | 875/1250 [08:21<03:09,  1.98it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  2.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-875\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-875\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-875/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-875/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-875/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-875/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-875/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-875/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-875/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-875/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-750] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-750] due to args.save_total_limit\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m70%|███████   | 876/1250 [08:25<14:16,  2.29s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 877/1250 [08:25<10:52,  1.75s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 878/1250 [08:26<08:37,  1.39s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 879/1250 [08:26<06:55,  1.12s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 880/1250 [08:27<05:44,  1.07it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 881/1250 [08:27<04:54,  1.25it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 882/1250 [08:28<04:19,  1.42it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 883/1250 [08:28<04:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 884/1250 [08:29<03:44,  1.63it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 885/1250 [08:29<03:30,  1.74it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 886/1250 [08:30<03:21,  1.81it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 887/1250 [08:30<03:14,  1.87it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 888/1250 [08:31<03:17,  1.83it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 889/1250 [08:31<03:13,  1.87it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 890/1250 [08:32<03:07,  1.92it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 891/1250 [08:32<03:03,  1.96it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 892/1250 [08:33<03:00,  1.99it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 893/1250 [08:33<03:05,  1.93it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 894/1250 [08:34<03:01,  1.97it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 895/1250 [08:34<02:58,  1.99it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 896/1250 [08:35<02:56,  2.01it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 897/1250 [08:35<03:02,  1.94it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 898/1250 [08:36<02:58,  1.97it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 899/1250 [08:36<02:55,  2.00it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 900/1250 [08:37<02:53,  2.01it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 901/1250 [08:37<02:52,  2.03it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 902/1250 [08:38<02:58,  1.95it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 903/1250 [08:38<02:55,  1.98it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 904/1250 [08:39<02:52,  2.01it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 905/1250 [08:39<02:51,  2.01it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 906/1250 [08:40<02:50,  2.01it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 907/1250 [08:40<02:57,  1.93it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 908/1250 [08:41<02:54,  1.96it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 909/1250 [08:41<02:51,  1.99it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 910/1250 [08:42<02:49,  2.01it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 911/1250 [08:42<02:55,  1.93it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 912/1250 [08:43<02:52,  1.96it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 913/1250 [08:43<02:49,  1.98it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 914/1250 [08:44<02:48,  2.00it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 915/1250 [08:44<02:46,  2.01it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 916/1250 [08:45<02:51,  1.95it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 917/1250 [08:45<02:47,  1.98it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 918/1250 [08:46<02:46,  1.99it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 919/1250 [08:46<02:45,  2.01it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 920/1250 [08:47<02:43,  2.02it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 921/1250 [08:47<02:50,  1.93it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 922/1250 [08:48<02:47,  1.96it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 923/1250 [08:48<02:44,  1.98it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 924/1250 [08:49<02:42,  2.00it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 925/1250 [08:49<02:41,  2.01it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 926/1250 [08:50<02:47,  1.93it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 927/1250 [08:50<02:45,  1.95it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 928/1250 [08:51<02:42,  1.98it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 929/1250 [08:51<02:41,  1.99it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 930/1250 [08:52<02:45,  1.93it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 931/1250 [08:52<02:42,  1.96it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 932/1250 [08:53<02:41,  1.97it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 933/1250 [08:53<02:39,  1.99it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 934/1250 [08:54<02:38,  1.99it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 935/1250 [08:55<02:42,  1.93it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 936/1250 [08:55<02:39,  1.96it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 937/1250 [08:56<02:37,  1.98it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 938/1250 [08:56<02:36,  1.99it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 939/1250 [08:57<02:35,  2.00it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 940/1250 [08:57<02:42,  1.90it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 941/1250 [08:58<02:39,  1.94it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 942/1250 [08:58<02:36,  1.97it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 943/1250 [08:59<02:34,  1.99it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 944/1250 [08:59<02:33,  2.00it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 945/1250 [09:00<02:38,  1.92it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 946/1250 [09:00<02:35,  1.95it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 947/1250 [09:01<02:33,  1.98it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 948/1250 [09:01<02:31,  1.99it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 949/1250 [09:02<02:36,  1.93it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 950/1250 [09:02<02:33,  1.96it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 951/1250 [09:03<02:30,  1.98it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 952/1250 [09:03<02:29,  2.00it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 953/1250 [09:04<02:28,  2.00it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 954/1250 [09:04<02:33,  1.92it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 955/1250 [09:05<02:31,  1.95it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 956/1250 [09:05<02:29,  1.97it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 957/1250 [09:06<02:27,  1.99it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 958/1250 [09:06<02:26,  2.00it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 959/1250 [09:07<02:31,  1.92it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 960/1250 [09:07<02:28,  1.95it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 961/1250 [09:08<02:25,  1.98it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 962/1250 [09:08<02:24,  1.99it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 963/1250 [09:09<02:23,  2.01it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 964/1250 [09:09<02:28,  1.92it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 965/1250 [09:10<02:27,  1.93it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 966/1250 [09:10<02:24,  1.96it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 967/1250 [09:11<02:22,  1.99it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 968/1250 [09:11<02:25,  1.93it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 969/1250 [09:12<02:23,  1.96it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 970/1250 [09:12<02:22,  1.97it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 971/1250 [09:13<02:21,  1.97it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 972/1250 [09:13<02:19,  1.99it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 973/1250 [09:14<02:24,  1.92it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 974/1250 [09:14<02:21,  1.96it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 975/1250 [09:15<02:18,  1.99it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 976/1250 [09:15<02:17,  2.00it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 977/1250 [09:16<02:18,  1.98it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 978/1250 [09:16<02:24,  1.88it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 979/1250 [09:17<02:21,  1.92it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 980/1250 [09:17<02:19,  1.93it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 981/1250 [09:18<02:17,  1.96it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 982/1250 [09:18<02:15,  1.98it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 983/1250 [09:19<02:18,  1.92it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 984/1250 [09:19<02:16,  1.96it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 985/1250 [09:20<02:13,  1.98it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 986/1250 [09:20<02:11,  2.00it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 987/1250 [09:21<02:15,  1.94it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 988/1250 [09:22<02:12,  1.97it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 989/1250 [09:22<02:11,  1.99it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 990/1250 [09:22<02:09,  2.00it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 991/1250 [09:23<02:08,  2.02it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 992/1250 [09:24<02:13,  1.94it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 993/1250 [09:24<02:10,  1.98it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 994/1250 [09:25<02:08,  1.99it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 995/1250 [09:25<02:07,  2.01it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 996/1250 [09:25<02:05,  2.02it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 997/1250 [09:26<02:10,  1.94it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 998/1250 [09:27<02:07,  1.97it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 999/1250 [09:27<02:05,  2.00it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1000/1250 [09:28<02:03,  2.02it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.104, 'learning_rate': 1.004e-05, 'epoch': 8.0}\u001b[0m\n",
      "\u001b[34m80%|████████  | 1000/1250 [09:28<02:03,  2.02it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 2000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34mNum examples = 2000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/8 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 2/8 [00:00<00:01,  4.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 3/8 [00:00<00:01,  3.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 4/8 [00:01<00:01,  3.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 5/8 [00:01<00:00,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 6/8 [00:01<00:00,  3.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 7/8 [00:02<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  2.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.14760388433933258, 'eval_accuracy': 0.9375, 'eval_f1': 0.9381526552039274, 'eval_precision': 0.9403978737090453, 'eval_recall': 0.9375, 'eval_runtime': 2.8685, 'eval_samples_per_second': 697.238, 'eval_steps_per_second': 2.789, 'epoch': 8.0}\u001b[0m\n",
      "\u001b[34m80%|████████  | 1000/1250 [09:30<02:03,  2.02it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 8/8 [00:02<00:00,  2.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-1000\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-1000\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-1000/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-1000/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-1000/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-1000/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-1000/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-1000/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1000/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1000/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m80%|████████  | 1001/1250 [09:34<09:09,  2.21s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1002/1250 [09:34<06:59,  1.69s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1003/1250 [09:35<05:28,  1.33s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1004/1250 [09:35<04:30,  1.10s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1005/1250 [09:36<03:44,  1.09it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1006/1250 [09:36<03:12,  1.27it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1007/1250 [09:37<02:49,  1.44it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1008/1250 [09:37<02:37,  1.53it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1009/1250 [09:38<02:25,  1.66it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1010/1250 [09:38<02:16,  1.76it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m81%|████████  | 1011/1250 [09:39<02:10,  1.83it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1012/1250 [09:39<02:05,  1.89it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1013/1250 [09:40<02:07,  1.86it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1014/1250 [09:40<02:03,  1.91it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1015/1250 [09:41<02:00,  1.95it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1016/1250 [09:41<01:58,  1.97it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1017/1250 [09:42<01:56,  2.00it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1018/1250 [09:42<02:00,  1.93it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1019/1250 [09:43<01:57,  1.96it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1020/1250 [09:43<01:55,  1.99it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1021/1250 [09:44<01:53,  2.01it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1022/1250 [09:44<01:52,  2.02it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1023/1250 [09:45<01:56,  1.94it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1024/1250 [09:45<01:54,  1.97it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1025/1250 [09:46<01:52,  2.00it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1026/1250 [09:46<01:51,  2.01it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1027/1250 [09:47<01:54,  1.95it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1028/1250 [09:47<01:54,  1.95it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1029/1250 [09:48<01:52,  1.97it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1030/1250 [09:48<01:50,  2.00it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1031/1250 [09:49<01:48,  2.02it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1032/1250 [09:49<01:52,  1.94it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1033/1250 [09:50<01:50,  1.97it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1034/1250 [09:50<01:48,  1.99it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1035/1250 [09:51<01:47,  2.01it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1036/1250 [09:51<01:45,  2.02it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1037/1250 [09:52<01:48,  1.97it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1038/1250 [09:52<01:48,  1.96it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1039/1250 [09:53<01:46,  1.99it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1040/1250 [09:53<01:44,  2.01it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1041/1250 [09:54<01:45,  1.99it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1042/1250 [09:54<01:47,  1.94it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1043/1250 [09:55<01:46,  1.94it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1044/1250 [09:55<01:44,  1.98it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1045/1250 [09:56<01:44,  1.96it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1046/1250 [09:56<01:49,  1.87it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1047/1250 [09:57<01:45,  1.93it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1048/1250 [09:57<01:42,  1.97it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1049/1250 [09:58<01:40,  2.00it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1050/1250 [09:58<01:39,  2.02it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1051/1250 [09:59<01:43,  1.92it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1052/1250 [09:59<01:40,  1.97it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1053/1250 [10:00<01:38,  2.00it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1054/1250 [10:00<01:39,  1.98it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1055/1250 [10:01<01:37,  2.01it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1056/1250 [10:02<01:39,  1.95it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1057/1250 [10:02<01:37,  1.99it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1058/1250 [10:02<01:35,  2.02it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1059/1250 [10:03<01:35,  1.99it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1060/1250 [10:04<01:38,  1.93it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1061/1250 [10:04<01:38,  1.93it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1062/1250 [10:05<01:37,  1.92it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1063/1250 [10:05<01:38,  1.90it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1064/1250 [10:06<01:36,  1.93it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1065/1250 [10:06<01:38,  1.88it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1066/1250 [10:07<01:35,  1.92it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1067/1250 [10:07<01:33,  1.95it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1068/1250 [10:08<01:32,  1.97it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1069/1250 [10:08<01:31,  1.98it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1070/1250 [10:09<01:35,  1.89it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1071/1250 [10:09<01:32,  1.93it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1072/1250 [10:10<01:30,  1.96it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1073/1250 [10:10<01:29,  1.97it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1074/1250 [10:11<01:28,  1.98it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1075/1250 [10:11<01:32,  1.89it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1076/1250 [10:12<01:30,  1.93it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1077/1250 [10:12<01:28,  1.95it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1078/1250 [10:13<01:27,  1.96it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1079/1250 [10:13<01:31,  1.88it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1080/1250 [10:14<01:28,  1.91it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1081/1250 [10:14<01:27,  1.93it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1082/1250 [10:15<01:26,  1.94it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1083/1250 [10:15<01:25,  1.95it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1084/1250 [10:16<01:28,  1.88it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1085/1250 [10:17<01:26,  1.90it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1086/1250 [10:17<01:25,  1.92it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1087/1250 [10:18<01:23,  1.95it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1088/1250 [10:18<01:22,  1.98it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1089/1250 [10:19<01:24,  1.90it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1090/1250 [10:19<01:22,  1.94it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1091/1250 [10:20<01:20,  1.97it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1092/1250 [10:20<01:19,  1.99it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1093/1250 [10:21<01:18,  2.01it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1094/1250 [10:21<01:21,  1.92it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1095/1250 [10:22<01:19,  1.95it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1096/1250 [10:22<01:19,  1.94it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1097/1250 [10:23<01:17,  1.96it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1098/1250 [10:23<01:19,  1.91it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1099/1250 [10:24<01:17,  1.94it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1100/1250 [10:24<01:16,  1.96it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1101/1250 [10:25<01:15,  1.98it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1102/1250 [10:25<01:14,  1.99it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1103/1250 [10:26<01:17,  1.89it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1104/1250 [10:26<01:15,  1.93it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1105/1250 [10:27<01:14,  1.95it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1106/1250 [10:27<01:12,  1.97it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 1107/1250 [10:28<01:11,  1.99it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 1108/1250 [10:28<01:14,  1.91it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 1109/1250 [10:29<01:12,  1.94it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1110/1250 [10:29<01:11,  1.97it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1111/1250 [10:30<01:10,  1.98it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1112/1250 [10:30<01:09,  1.99it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1113/1250 [10:31<01:11,  1.91it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1114/1250 [10:31<01:09,  1.94it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1115/1250 [10:32<01:08,  1.96it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1116/1250 [10:32<01:07,  1.98it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1117/1250 [10:33<01:09,  1.92it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1118/1250 [10:33<01:07,  1.95it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1119/1250 [10:34<01:06,  1.97it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1120/1250 [10:34<01:05,  1.98it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1121/1250 [10:35<01:04,  1.99it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1122/1250 [10:35<01:07,  1.91it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1123/1250 [10:36<01:05,  1.94it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1124/1250 [10:36<01:05,  1.92it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1125/1250 [10:37<01:04,  1.95it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/8 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 2/8 [00:00<00:01,  4.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 3/8 [00:00<00:01,  3.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 4/8 [00:01<00:01,  3.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 5/8 [00:01<00:00,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 6/8 [00:01<00:00,  3.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 7/8 [00:02<00:00,  2.73it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1530248075723648, 'eval_accuracy': 0.9345, 'eval_f1': 0.9345077978012077, 'eval_precision': 0.9346316724320993, 'eval_recall': 0.9345, 'eval_runtime': 2.9642, 'eval_samples_per_second': 674.719, 'eval_steps_per_second': 2.699, 'epoch': 9.0}\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1125/1250 [10:40<01:04,  1.95it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 8/8 [00:02<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-1125\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-1125\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-1125/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-1125/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-1125/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-1125/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-1125/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-1125/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1125/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1125/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-1000] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-1000] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1126/1250 [10:44<04:49,  2.34s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1127/1250 [10:44<03:39,  1.78s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1128/1250 [10:45<02:50,  1.40s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1129/1250 [10:45<02:18,  1.15s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1130/1250 [10:46<01:53,  1.05it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1131/1250 [10:46<01:36,  1.23it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1132/1250 [10:47<01:24,  1.40it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1133/1250 [10:47<01:15,  1.55it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1134/1250 [10:48<01:12,  1.61it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1135/1250 [10:48<01:06,  1.72it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m91%|█████████ | 1136/1250 [10:49<01:03,  1.81it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1137/1250 [10:49<01:00,  1.86it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1138/1250 [10:50<01:00,  1.85it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1139/1250 [10:50<00:58,  1.90it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1140/1250 [10:51<00:57,  1.90it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 1141/1250 [10:51<00:55,  1.95it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 1142/1250 [10:52<00:54,  1.97it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 1143/1250 [10:52<00:56,  1.91it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1144/1250 [10:53<00:54,  1.95it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1145/1250 [10:53<00:52,  1.99it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1146/1250 [10:54<00:52,  1.98it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1147/1250 [10:54<00:51,  2.01it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1148/1250 [10:55<00:52,  1.93it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1149/1250 [10:55<00:52,  1.93it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1150/1250 [10:56<00:51,  1.94it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1151/1250 [10:56<00:50,  1.95it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1152/1250 [10:57<00:50,  1.95it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1153/1250 [10:57<00:50,  1.92it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1154/1250 [10:58<00:49,  1.94it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1155/1250 [10:58<00:47,  1.99it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1156/1250 [10:59<00:47,  1.99it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1157/1250 [10:59<00:48,  1.91it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1158/1250 [11:00<00:47,  1.96it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1159/1250 [11:00<00:45,  1.99it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1160/1250 [11:01<00:45,  1.99it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1161/1250 [11:01<00:44,  2.01it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1162/1250 [11:02<00:45,  1.93it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1163/1250 [11:02<00:44,  1.97it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1164/1250 [11:03<00:42,  2.01it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1165/1250 [11:03<00:41,  2.03it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1166/1250 [11:04<00:41,  2.02it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1167/1250 [11:04<00:42,  1.97it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1168/1250 [11:05<00:41,  1.98it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 1169/1250 [11:05<00:40,  2.00it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 1170/1250 [11:06<00:40,  2.00it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 1171/1250 [11:06<00:39,  1.99it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1172/1250 [11:07<00:40,  1.92it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1173/1250 [11:07<00:39,  1.96it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1174/1250 [11:08<00:38,  1.96it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1175/1250 [11:08<00:38,  1.96it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1176/1250 [11:09<00:38,  1.93it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1177/1250 [11:09<00:37,  1.96it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1178/1250 [11:10<00:36,  1.99it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1179/1250 [11:10<00:35,  1.98it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1180/1250 [11:11<00:34,  2.01it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1181/1250 [11:12<00:35,  1.93it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1182/1250 [11:12<00:34,  1.96it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1183/1250 [11:13<00:33,  1.98it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1184/1250 [11:13<00:33,  1.99it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1185/1250 [11:13<00:32,  2.01it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1186/1250 [11:14<00:32,  1.95it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1187/1250 [11:15<00:31,  1.99it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1188/1250 [11:15<00:30,  2.01it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1189/1250 [11:16<00:30,  1.99it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1190/1250 [11:16<00:31,  1.88it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1191/1250 [11:17<00:30,  1.93it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1192/1250 [11:17<00:29,  1.95it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1193/1250 [11:18<00:28,  1.98it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1194/1250 [11:18<00:28,  1.97it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1195/1250 [11:19<00:29,  1.88it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1196/1250 [11:19<00:27,  1.94it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1197/1250 [11:20<00:27,  1.95it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1198/1250 [11:20<00:26,  1.96it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1199/1250 [11:21<00:25,  1.98it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1200/1250 [11:21<00:26,  1.90it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1201/1250 [11:22<00:25,  1.95it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1202/1250 [11:22<00:24,  1.98it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1203/1250 [11:23<00:23,  2.00it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 1204/1250 [11:23<00:23,  1.99it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 1205/1250 [11:24<00:23,  1.93it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 1206/1250 [11:24<00:22,  1.96it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1207/1250 [11:25<00:22,  1.95it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1208/1250 [11:25<00:21,  1.98it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1209/1250 [11:26<00:21,  1.93it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1210/1250 [11:26<00:20,  1.93it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1211/1250 [11:27<00:19,  1.97it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1212/1250 [11:27<00:19,  1.98it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1213/1250 [11:28<00:18,  2.00it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1214/1250 [11:28<00:18,  1.90it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1215/1250 [11:29<00:18,  1.94it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1216/1250 [11:29<00:17,  1.97it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1217/1250 [11:30<00:16,  1.99it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1218/1250 [11:30<00:16,  1.97it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1219/1250 [11:31<00:16,  1.92it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1220/1250 [11:31<00:15,  1.91it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1221/1250 [11:32<00:14,  1.94it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1222/1250 [11:32<00:14,  1.94it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1223/1250 [11:33<00:13,  1.97it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1224/1250 [11:34<00:13,  1.92it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1225/1250 [11:34<00:12,  1.93it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1226/1250 [11:35<00:12,  1.94it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1227/1250 [11:35<00:11,  1.97it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1228/1250 [11:36<00:11,  1.93it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1229/1250 [11:36<00:10,  1.94it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1230/1250 [11:37<00:10,  1.94it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1231/1250 [11:37<00:09,  1.97it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 1232/1250 [11:38<00:09,  1.99it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 1233/1250 [11:38<00:08,  1.94it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 1234/1250 [11:39<00:08,  1.94it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1235/1250 [11:39<00:07,  1.97it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1236/1250 [11:40<00:07,  1.95it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1237/1250 [11:40<00:06,  1.98it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1238/1250 [11:41<00:06,  1.93it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1239/1250 [11:41<00:05,  1.92it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1240/1250 [11:42<00:05,  1.93it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1241/1250 [11:42<00:04,  1.97it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1242/1250 [11:43<00:04,  1.96it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1243/1250 [11:43<00:03,  1.92it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1244/1250 [11:44<00:03,  1.93it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1245/1250 [11:44<00:02,  1.94it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1246/1250 [11:45<00:02,  1.94it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1247/1250 [11:45<00:01,  1.86it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1248/1250 [11:46<00:01,  1.89it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1249/1250 [11:46<00:00,  1.90it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1250/1250 [11:47<00:00,  1.91it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/8 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 2/8 [00:00<00:01,  4.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 3/8 [00:00<00:01,  3.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 4/8 [00:01<00:01,  3.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 5/8 [00:01<00:00,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 6/8 [00:01<00:00,  3.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 7/8 [00:02<00:00,  2.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  2.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1613253802061081, 'eval_accuracy': 0.941, 'eval_f1': 0.9413321705151999, 'eval_precision': 0.9419519436781406, 'eval_recall': 0.941, 'eval_runtime': 2.8864, 'eval_samples_per_second': 692.901, 'eval_steps_per_second': 2.772, 'epoch': 10.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 1250/1250 [11:50<00:00,  1.91it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 8/8 [00:02<00:00,  2.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-1250\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-1250\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-1250/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-1250/config.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-12-03 21:18:52 Uploading - Uploading generated training model\u001b[34mModel weights saved in /opt/ml/model/checkpoint-1250/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-1250/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-1250/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-1250/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1250/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1250/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-875] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-875] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-1125] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-1125] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mLoading best model from /opt/ml/model/checkpoint-1250 (score: 0.9413321705151999).\u001b[0m\n",
      "\u001b[34mLoading best model from /opt/ml/model/checkpoint-1250 (score: 0.9413321705151999).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 713.8587, 'train_samples_per_second': 224.134, 'train_steps_per_second': 1.751, 'train_loss': 0.3196483932495117, 'epoch': 10.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 1250/1250 [11:53<00:00,  1.91it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1250/1250 [11:53<00:00,  1.75it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2000\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34mBatch size = 256\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 2/8 [00:00<00:01,  5.78it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 3/8 [00:00<00:01,  4.00it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 4/8 [00:01<00:01,  3.13it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 5/8 [00:01<00:00,  3.05it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 6/8 [00:01<00:00,  2.97it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 7/8 [00:02<00:00,  2.94it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  3.01it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 8/8 [00:02<00:00,  3.20it/s]\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2021-12-03 21:18:50,928 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-12-03 21:23:57 Completed - Training job completed\n",
      "Training seconds: 1353\n",
      "Billable seconds: 1353\n",
      "1565.9995799064636\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'val': val_input_path})\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27393259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding waiter to see when training is done\n",
    "waiter = huggingface_estimator.sagemaker_session.sagemaker_client.get_waiter('training_job_completed_or_stopped')\n",
    "waiter.wait(TrainingJobName=huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6faf794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "# # Captured metrics can be accessed as a Pandas dataframe\n",
    "# df = TrainingJobAnalytics(training_job_name=huggingface_estimator.latest_training_job.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad179292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://emotion-class-197614225699/training_output/emotion-training-2021-12-03-20-58-03-225/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(huggingface_estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc15303",
   "metadata": {},
   "source": [
    "### Save training job name for next session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3813f6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emotion-training-2021-12-03-20-58-03-225'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_job_name = huggingface_estimator.latest_training_job.name\n",
    "training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "659c1b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'training_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833ea5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Upload the fine-tuned model to huggingface.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "736394c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface\n",
    "import botocore\n",
    "# from datasets.filesystems import S3FileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1da8623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.68.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d7e43e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::197614225699:role/bi-sagemaker-access\n",
      "sagemaker bucket: sagemaker-us-east-1-197614225699\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "#sagemaker_session_bucket=\"samsum-dataset\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2947c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "local_path = 'my_roberta_base_emotion'\n",
    "\n",
    "os.makedirs(local_path, exist_ok = True)\n",
    "\n",
    "# download model from S3\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path=local_path, # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess # sagemaker session used for training the model\n",
    ")\n",
    "\n",
    "# unzip model\n",
    "tar = tarfile.open(f\"{local_path}/model.tar.gz\", \"r:gz\")\n",
    "tar.extractall(path=local_path)\n",
    "tar.close()\n",
    "os.remove(f\"{local_path}/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04841198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta\n",
    "import os\n",
    "import math\n",
    "import warnings \n",
    "from datetime import datetime, date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7fbaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read eval and test results \n",
    "# with open(f\"{local_path}/eval_results.json\") as f:\n",
    "#     eval_results_raw = json.load(f)\n",
    "#     print(eval_results_raw)\n",
    "#     df_results = pd.json_normalize(eval_results_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb938c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
